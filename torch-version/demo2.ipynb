{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python392jvsc74a57bd05c0cbb8f9505cb68e0576a6eaefb85eb32fb13a63ecd19ecdbf1ad8294a8a3f6",
   "display_name": "Python 3.9.2 64-bit ('base': conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "5c0cbb8f9505cb68e0576a6eaefb85eb32fb13a63ecd19ecdbf1ad8294a8a3f6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import FastText\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loading reviews Coast 0.1089 Sec\n",
      "Train Model Coast 7.2250 Sec\n"
     ]
    }
   ],
   "source": [
    "vector_size = 200\n",
    "# train fasttext model\n",
    "time_start = time.time()\n",
    "\n",
    "filePath = '../corpus/6moods/train/usual_trainTrimed.csv'\n",
    "df = pd.read_csv(filePath)\n",
    "labels, reviews = df['label'].astype('str'), df['review'].astype('str')\n",
    "reviews = [str(review).split() for review in reviews]\n",
    "\n",
    "print(f'Loading reviews Coast {time.time()-time_start:.4f} Sec')\n",
    "time_start = time.time()\n",
    "\n",
    "ft = FastText(reviews, vector_size=vector_size, epochs=20, window=2, min_count=5, min_n=2, max_n=4, word_ngrams=1, workers=8)\n",
    "\n",
    "print(f'Train Model Coast {time.time()-time_start:.4f} Sec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert label from str to int\n",
    "# mood dict\n",
    "m2i = {\n",
    "    'sad':0,\n",
    "    'angry': 1,\n",
    "    'fear': 2,\n",
    "    'neutral': 3,\n",
    "    'surprise': 4,\n",
    "    'happy': 5,\n",
    "}\n",
    "i2m = {k:i for k, i in enumerate(m2i)}\n",
    "labels = [m2i[label] for label in labels]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word2vec and padding\n",
    "vectors = [[ft.wv[word] for word in review] for review in reviews]\n",
    "max_len = max([len(vector) for vector in vectors])\n",
    "zeros = [0 for i in range(vector_size)]\n",
    "for i in range(len(vectors)):\n",
    "    while len(vectors[i]) < max_len:\n",
    "        vectors[i].insert(0, zeros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(torch.Size([27768, 80, 200]), torch.Size([27768]))"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "# reformat data\n",
    "def to_categorical(labels):\n",
    "    n = len(labels)\n",
    "    num_type = 6\n",
    "    res = [[0] * num_type for i in range(n)]\n",
    "    for i in range(n):\n",
    "        res[i][labels[i]] = 1\n",
    "    return res\n",
    "\n",
    "vectors = torch.tensor(vectors)\n",
    "# labels = to_categorical(labels)\n",
    "labels = torch.tensor(labels, dtype=int)\n",
    "vectors.shape, labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(19437, 8331)"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "# 打乱并分割数据集\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# random_state表示随机数种子\n",
    "labels_train, labels_test, vectors_train, vectors_test = train_test_split(\n",
    "    labels, vectors, \n",
    "    test_size=0.3, random_state=0\n",
    ")\n",
    "len(vectors_train), len(vectors_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SA(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SA, self).__init__()\n",
    "        self.input_size = 200\n",
    "        self.hidden_size = 256\n",
    "        self.num_layers = 2\n",
    "        self.fch1_size = 64\n",
    "        self.fch2_size = 16\n",
    "        self.output_size = 6\n",
    "        self.lstm = nn.LSTM(self.input_size, self.hidden_size, self.num_layers)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(self.hidden_size, self.fch1_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.fch1_size, self.fch2_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.fch2_size, self.output_size),\n",
    "        )\n",
    "        self.fc = nn.Linear(self.hidden_size, self.output_size)\n",
    "    def forward(self, input):\n",
    "        out, hidden = self.lstm(input)\n",
    "        # 只取最后一个输出\n",
    "        out = torch.squeeze(out[:,-1:])\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "SA(\n  (lstm): LSTM(200, 256, num_layers=2)\n  (fc): Linear(in_features=256, out_features=6, bias=True)\n)\n"
     ]
    }
   ],
   "source": [
    "model = SA()\n",
    "print(model)"
   ]
  },
  {
   "source": [
    "def train(X, y, model, loss_fn, optimizer):\n",
    "    n = len(X)\n",
    "    start = 0\n",
    "    while start < n:\n",
    "        X_sample = X[start:min(start+batch_size, n)]\n",
    "        y_sample = y[start:min(start+batch_size, n)]\n",
    "        pred = model(X_sample)\n",
    "        # print(pred.shape, y_sample.shape)\n",
    "        loss = loss_fn(pred, y_sample)\n",
    "\n",
    "        # backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        start += batch_size\n",
    "        if start % 100 == 0:\n",
    "            loss, current = loss.item(), start\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{n:>5d}]\")"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(X, y, model):\n",
    "    n = len(X)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        pred = model(X)\n",
    "        test_loss = loss_fn(pred, y).sum().item()\n",
    "        correct = (pred.argmax(1) == y).sum().item()\n",
    "    # test_loss /= n\n",
    "    correct /= n\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 1.690273  [ 1600/19437]\n",
      "loss: 1.560850  [ 3200/19437]\n",
      "loss: 1.603476  [ 4800/19437]\n",
      "loss: 1.444831  [ 6400/19437]\n",
      "loss: 1.587663  [ 8000/19437]\n",
      "loss: 1.372534  [ 9600/19437]\n",
      "loss: 1.561523  [11200/19437]\n",
      "loss: 1.387775  [12800/19437]\n",
      "loss: 1.425275  [14400/19437]\n",
      "loss: 1.407422  [16000/19437]\n",
      "loss: 1.577097  [17600/19437]\n",
      "loss: 1.287098  [19200/19437]\n",
      "Test Error: \n",
      " Accuracy: 46.1%, loss: 1.398483 \n",
      "\n",
      "Cost time: 183.9810\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 1.510795  [ 1600/19437]\n",
      "loss: 1.278479  [ 3200/19437]\n",
      "loss: 1.438341  [ 4800/19437]\n",
      "loss: 1.294572  [ 6400/19437]\n",
      "loss: 1.573930  [ 8000/19437]\n",
      "loss: 1.293692  [ 9600/19437]\n",
      "loss: 1.547958  [11200/19437]\n",
      "loss: 1.365297  [12800/19437]\n",
      "loss: 1.423805  [14400/19437]\n",
      "loss: 1.390561  [16000/19437]\n",
      "loss: 1.529966  [17600/19437]\n",
      "loss: 1.230670  [19200/19437]\n",
      "Test Error: \n",
      " Accuracy: 47.3%, loss: 1.382396 \n",
      "\n",
      "Cost time: 228.4507\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.483703  [ 1600/19437]\n",
      "loss: 1.221874  [ 3200/19437]\n",
      "loss: 1.398335  [ 4800/19437]\n",
      "loss: 1.270951  [ 6400/19437]\n",
      "loss: 1.547704  [ 8000/19437]\n",
      "loss: 1.262360  [ 9600/19437]\n",
      "loss: 1.534580  [11200/19437]\n",
      "loss: 1.338344  [12800/19437]\n",
      "loss: 1.412958  [14400/19437]\n",
      "loss: 1.372073  [16000/19437]\n",
      "loss: 1.486634  [17600/19437]\n",
      "loss: 1.194623  [19200/19437]\n",
      "Test Error: \n",
      " Accuracy: 47.7%, loss: 1.386138 \n",
      "\n",
      "Cost time: 263.0563\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.458993  [ 1600/19437]\n",
      "loss: 1.188557  [ 3200/19437]\n",
      "loss: 1.363984  [ 4800/19437]\n",
      "loss: 1.252696  [ 6400/19437]\n",
      "loss: 1.512194  [ 8000/19437]\n",
      "loss: 1.220892  [ 9600/19437]\n",
      "loss: 1.509326  [11200/19437]\n",
      "loss: 1.297835  [12800/19437]\n",
      "loss: 1.389122  [14400/19437]\n",
      "loss: 1.343353  [16000/19437]\n",
      "loss: 1.421889  [17600/19437]\n",
      "loss: 1.161031  [19200/19437]\n",
      "Test Error: \n",
      " Accuracy: 47.0%, loss: 1.404867 \n",
      "\n",
      "Cost time: 235.8416\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.413034  [ 1600/19437]\n",
      "loss: 1.149598  [ 3200/19437]\n",
      "loss: 1.337931  [ 4800/19437]\n",
      "loss: 1.223611  [ 6400/19437]\n",
      "loss: 1.451945  [ 8000/19437]\n",
      "loss: 1.160267  [ 9600/19437]\n",
      "loss: 1.453645  [11200/19437]\n",
      "loss: 1.223064  [12800/19437]\n",
      "loss: 1.331524  [14400/19437]\n",
      "loss: 1.280920  [16000/19437]\n",
      "loss: 1.323161  [17600/19437]\n",
      "loss: 1.092805  [19200/19437]\n",
      "Test Error: \n",
      " Accuracy: 45.7%, loss: 1.458275 \n",
      "\n",
      "Cost time: 235.2089\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "batch_size = 64\n",
    "epochs = 5\n",
    "for t in range(epochs):\n",
    "    start = time.time()\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(vectors_train, labels_train, model, loss_fn, optimizer)\n",
    "    test(vectors_test, labels_test, model)\n",
    "    print(f'Cost time: {time.time() - start:.4f}')\n",
    "print(\"Done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[-0.0317, -0.2670, -1.2810, -0.7244, -1.2785,  2.6586],\n        [ 1.1599,  0.3993, -2.3770, -0.5475,  0.6775,  0.0266]],\n       grad_fn=<AddmmBackward>) torch.Size([2, 6]) torch.Size([2])\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor(0.8111, grad_fn=<NllLossBackward>)"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "pred = model(vectors_train[:2])\n",
    "print(pred, pred.shape, labels_train[:2].shape)\n",
    "loss = loss_fn(pred, labels_train[:2])\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}